{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.nuscenes import compile_data as compile_data, denormalize_img\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipped: False\n",
      "Dims: (224, 480)\n",
      "../data/nuscenes/trainval\n",
      "OOD labels: ['vehicle.bus.rigid', 'vehicle.bus.bendy']\n",
      "OOD labels: ['vehicle.bus.rigid', 'vehicle.bus.bendy']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./configs/eval_nuscenes_cvt_enn.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "config['batch_size'] = 1\n",
    "config['num_workers'] = 1\n",
    "\n",
    "train_loader, val_loader = compile_data(\"trainval\", config, shuffle_train=True, ood=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = iter(train_loader)\n",
    "next(t)\n",
    "imgs, rots, trans, intrins, extrins, post_rots, post_trans, labels, ood, ood_cam = next(t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw = [\n",
    "    (56, 120),\n",
    "    (14, 30),\n",
    "]\n",
    "\n",
    "H, W = 25, 25\n",
    "\n",
    "query_locations = [\n",
    "    (1, 1),\n",
    "    (4, 4)\n",
    "] # (Y, X)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CVT model\n",
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.cvt.cross_view_transformer import *\n",
    "\n",
    "cvt = torch.nn.DataParallel(CrossViewTransformer(outC=4), device_ids=[0])\n",
    "cvt.load_state_dict(torch.load(\"./nuscenes/cvt_enn/best_iou.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, atts = cvt(imgs, rots, trans, intrins, extrins, post_rots, post_trans, return_att=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"(b m) (H W) (n h w) -> b m H W n h w\".\n Input tensor shape: torch.Size([4, 625, 40320]). Additional info: {'n': 6, 'm': 1, 'H': 224, 'W': 480, 'h': 56, 'w': 120}.\n Shape mismatch, 625 != 107520",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/data/bny220000/projects/bev/venv/lib/python3.8/site-packages/einops/einops.py:412\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    411\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[38;5;241m=\u001b[39mhashable_axes_lengths)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/data/bny220000/projects/bev/venv/lib/python3.8/site-packages/einops/einops.py:235\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[1;32m    233\u001b[0m backend \u001b[38;5;241m=\u001b[39m get_backend(tensor)\n\u001b[1;32m    234\u001b[0m init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 235\u001b[0m     \u001b[43m_reconstruct_from_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m tensor \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mreshape(tensor, init_shapes)\n",
      "File \u001b[0;32m/data/bny220000/projects/bev/venv/lib/python3.8/site-packages/einops/einops.py:191\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(length, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(known_product, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m length \u001b[38;5;241m!=\u001b[39m known_product:\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape mismatch, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(length, known_product))\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# this is enforced when recipe is created\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# elif len(unknown_axes) > 1:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m#     raise EinopsError(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, 625 != 107520",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m rearranged_atts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, att \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(atts):\n\u001b[0;32m----> 3\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(b m) (H W) (n h w) -> b m H W n h w\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m             \n\u001b[1;32m      5\u001b[0m     rearranged_atts\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n",
      "File \u001b[0;32m/data/bny220000/projects/bev/venv/lib/python3.8/site-packages/einops/einops.py:483\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRearrange can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be applied to an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    482\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m get_backend(tensor[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstack_on_zeroth_dimension(tensor)\n\u001b[0;32m--> 483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/bny220000/projects/bev/venv/lib/python3.8/site-packages/einops/einops.py:420\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    418\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Input is list. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    419\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"(b m) (H W) (n h w) -> b m H W n h w\".\n Input tensor shape: torch.Size([4, 625, 40320]). Additional info: {'n': 6, 'm': 1, 'H': 224, 'W': 480, 'h': 56, 'w': 120}.\n Shape mismatch, 625 != 107520"
     ]
    }
   ],
   "source": [
    "\n",
    "rearranged_atts = []\n",
    "for i, att in enumerate(atts):\n",
    "    r = rearrange(att, '(b m) (H W) (n h w) -> b m H W n h w', n=6, m=4, H=H, W=W, h=hw[i][0],\n",
    "                              w=hw[i][1])\n",
    "    rearranged_atts.append(r)\n",
    "\n",
    "# Visualization\n",
    "att = rearranged_atts[-1]  # get the last attention layer\n",
    "att = att[0].detach()  # batch size is 1, we remove the batch dim here.\n",
    "mean_att = torch.mean(att, dim=0)\n",
    "preds = preds.softmax(dim=1)\n",
    "pred, labels = save_pred(preds, labels, './')\n",
    "\n",
    "cam_img = np.transpose(imgs[0][1].cpu().numpy(), (1, 2, 0))\n",
    "x, y, k = map(mean_att, 1, 1, 20)\n",
    "\n",
    "cv2.imwrite(\"labels.jpg\", labels)\n",
    "cv2.imwrite(\"fc.jpg\", cam_img*255)\n",
    "\n",
    "for query_location in query_locations:\n",
    "    att_map = mean_att[query_location[0]][query_location[1]].detach().cpu().numpy()\n",
    "\n",
    "    pred_img = preds[0][0].detach().cpu().numpy()\n",
    "    plt.title('Prediction and Query')\n",
    "    plt.imshow(pred_img)\n",
    "    # Query mask\n",
    "    query_mask = np.zeros((H, W))\n",
    "    query_mask[query_location[0], query_location[1]] = 1\n",
    "    query_mask = cv2.resize(query_mask, dsize=(pred_img.shape[1], pred_img.shape[0]),\n",
    "                            interpolation=cv2.INTER_NEAREST)\n",
    "    plt.imshow(query_mask, alpha=query_mask)\n",
    "    plt.savefig(\"query_mask.jpg\")\n",
    "\n",
    "    ova = np.unravel_index(att_map.argmax(), att_map.shape)\n",
    "    print(ova)\n",
    "    print(att_map[ova[0], ova[1], ova[2]])\n",
    "\n",
    "    for j in range(6):\n",
    "        cam_img = np.transpose(imgs[0][j].cpu().numpy(), (1, 2, 0))\n",
    "        att_map = mean_att[query_location[0]][query_location[1]][j].cpu().numpy()\n",
    "        plt.title(f'Cam {j}')\n",
    "        plt.imshow(cam_img)\n",
    "        # att_map *= 10\n",
    "        att_map_upsampled = cv2.resize(att_map, dsize=(cam_img.shape[1], cam_img.shape[0]),\n",
    "                                       interpolation=cv2.INTER_NEAREST)\n",
    "        plt.imshow(att_map_upsampled, alpha=att_map_upsampled+.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
